{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94001c84-560b-4c1f-92f6-55efdabe0c59",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9226158-94b8-42f2-925e-eea372cf2d03",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe293a74-5fed-4cb9-b875-89dcb9d83147",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b40b95-6ca2-442a-923f-6d9dd8e9a7f0",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It's an ensemble learning method that combines the predictions of multiple weak regression models (typically decision trees) to create a strong predictive model. Gradient Boosting Regression belongs to the broader family of boosting algorithms, and it's particularly effective for solving regression problems where the goal is to predict continuous numerical values rather than discrete categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b486a4-88b9-4016-b997-5ac2bb7f0196",
   "metadata": {},
   "source": [
    "#### Initialization:\n",
    "\n",
    "- An initial model is created, usually a simple one like the mean or median of the target variable. This initial model serves as a starting point for the ensemble.\n",
    "\n",
    "#### Sequential Training of Weak Learners:\n",
    "\n",
    "- Gradient Boosting Regression trains a series of weak regression models sequentially.\n",
    "- In each iteration, a new weak learner (typically a shallow decision tree) is trained to predict the residuals or errors from the previous ensemble's predictions.\n",
    "- The residuals represent the difference between the actual target values and the predictions made by the current ensemble. The new weak learner is trained to capture the patterns in these residuals.\n",
    "- The learning rate, which controls the contribution of each weak learner to the ensemble, can be adjusted to make the process more conservative or aggressive.\n",
    "\n",
    "#### Combination of Weak Learners:\n",
    "\n",
    "- After each iteration, the predictions of the new weak learner are combined with the predictions of the previous ensemble.\n",
    "- The combination is done by adding the new weak learner's predictions (scaled by the learning rate) to the previous ensemble's predictions.\n",
    "\n",
    "#### Sequential Iteration:\n",
    "\n",
    "- Steps 2 and 3 are repeated for a predefined number of iterations or until a certain performance criterion is met.\n",
    "- With each iteration, the ensemble becomes more accurate in modeling the relationship between the input features and the target variable.\n",
    "\n",
    "#### Final Model Output:\n",
    "\n",
    "- The final prediction for a given input is the sum of the initial model's prediction and the predictions of all the weak learners in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcec7f0-59cb-43fc-990f-8cf682e17ca9",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0589763d-f51f-4d22-996a-5626690494de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ba8f6d-e53e-490f-b3d4-856998381f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc06fac1-9ca7-442b-aa74-e44a65486cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0e3c98-bc6a-4e41-8963-d471fd644a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.prspecieseprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb053545-a1d9-457f-a934-09935db0111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species']=encoder.fit_transform(df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f9198b-d625-427b-91a1-46df8a2c118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f82990f-c010-43ab-9cdf-2be3ee0157cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e7718a-cc73-43b2-86b3-b6e4f733e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr=GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0420e8-383f-4058-91a6-9e976e54dba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e8bac73-de6d-4a47-800a-8a1be129cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4f0919f-091b-4d40-9303-a3a8920da49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b20844d0-3f35-4157-8cd9-afa24087a940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9957266014015651"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c340fbf0-398b-48ce-87cb-90f801f5a869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00300972740623842"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ac9d4-5b4c-461e-b288-bb58d6e0379f",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00a308f3-d80a-4490-a249-abcce426da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "param={ 'n_estimators':[100,200,300],\n",
    "       \"max_depth\":[1,2,3],\n",
    "       \"alpha\":[.5,.2,.9]\n",
    "       \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a282389c-f2bd-4f0c-bfe6-8f1f09a7aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d97ce1c-ec24-4a71-b2bc-06e1d3db1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gbr=GridSearchCV(GradientBoostingRegressor(),param_grid=param,cv=5,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6bcf6f6-56b3-48c9-a240-a4bd58171936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.5, 0.2, 0.9], &#x27;max_depth&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.5, 0.2, 0.9], &#x27;max_depth&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;r2&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'alpha': [0.5, 0.2, 0.9], 'max_depth': [1, 2, 3],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3dba2bf-ccbe-4828-b24f-e6c1afc187cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.2, 'max_depth': 1, 'n_estimators': 100}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gbr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1b4250d-e315-4177-b18d-d2f9e7c4380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gbr=GradientBoostingRegressor(alpha=0.2,max_depth=1,n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38a9b640-7ebd-4096-ad4a-6bb71ab64f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(alpha=0.2, max_depth=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(alpha=0.2, max_depth=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.2, max_depth=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_gbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3794d531-3d84-4bdb-abe2-2108659d6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new=new_gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9240095d-123a-4f5a-8b97-e651f11d998b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866286382623181"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee4267ad-efb5-4dab-bfd4-524f5c8424b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009417364880348025"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,y_pred_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d1fed-5efc-4ca1-9811-5a0de0ae9e97",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f6a3a-b1d2-40ce-812d-4e3b006f7751",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a simple, often shallow, and less expressive model that performs slightly better than random chance on the task at hand. Weak learners are also sometimes referred to as base learners. These models are typically simple decision trees, linear models, or other simple models.\n",
    "\n",
    "The concept of using weak learners in Gradient Boosting is a key element of the algorithm's success. Here are some characteristics of weak learners in Gradient Boosting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebf30a-3df8-4ec3-a721-80d82535ad66",
   "metadata": {},
   "source": [
    "1. Low Complexity: Weak learners are deliberately kept simple and have low complexity. For decision trees, they are often shallow with a limited number of nodes and a small maximum depth.\n",
    "\n",
    "2. Low Bias, High Variance: Weak learners have low bias, meaning they can fit the training data relatively well. However, they have high variance, which means they are sensitive to small changes in the training data and can overfit if used independently.\n",
    "\n",
    "3. Slightly Better Than Random: Weak learners should perform slightly better than random guessing. In binary classification, this means they should have an accuracy better than 50%, and in regression, they should have a predictive performance better than predicting the mean of the target variable.\n",
    "\n",
    "4. Fast to Train: Weak learners are typically computationally efficient and quick to train. This is essential because Gradient Boosting involves sequentially training many weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7decd-7987-4185-88bd-7604666ce76f",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33cbf64-fd22-46ac-b497-178111e113c1",
   "metadata": {},
   "source": [
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by sequentially combining the outputs of multiple weak models (typically simple decision trees) in a way that focuses on correcting the errors made by the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03556ff8-345a-43a7-bdd2-f1112f445196",
   "metadata": {},
   "source": [
    "#### Start with a Simple Model:\n",
    "Gradient Boosting begins with a simple model, often just the mean (for regression) or a constant (for classification). This model serves as an initial approximation of the target variable or class distribution.\n",
    "\n",
    "#### Iteratively Correct Errors:\n",
    "The algorithm proceeds iteratively, where each iteration focuses on improving the model's predictions. In each iteration:\n",
    "\n",
    "- A new weak learner (decision tree) is trained to predict the residuals or errors of the previous model. These residuals represent the differences between the actual target values and the current model's predictions.\n",
    "- The new weak learner is chosen to minimize the errors (residuals) made by the previous model, effectively correcting the model's mistakes.\n",
    "- The learning rate, which controls the contribution of each weak learner, can be adjusted to make the updates more conservative or aggressive.\n",
    "\n",
    "#### Combine Weak Learners:\n",
    "The predictions of each weak learner are combined in a weighted manner to update the overall model. Weak learners that perform better at correcting the model's errors are given higher weights in the combination.\n",
    "\n",
    "#### Gradual Improvement: \n",
    "Over multiple iterations, the ensemble of weak learners gradually improves its predictive performance by reducing the errors in each iteration. The final model combines the strengths of all the weak learners to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a0d4b-0c77-48bb-951c-2424026f1b46",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b034731-3aa0-4c4e-8218-60191f3a2c47",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners iteratively to create a strong predictive model.\n",
    "\n",
    "#### Initialization:\n",
    "\n",
    "The ensemble starts with an initial model, which is often a simple one, such as the mean (for regression) or a constant (for classification). This initial model serves as the starting point for the ensemble.\n",
    "\n",
    "#### Sequential Training of Weak Learners:\n",
    "\n",
    "- Gradient Boosting trains a series of weak learners sequentially, one after the other. Each iteration focuses on improving the model's predictions.\n",
    "- In each iteration, a new weak learner (typically a shallow decision tree) is trained to predict the residuals or errors from the previous ensemble's predictions. These residuals represent the differences between the actual target values and the current model's predictions.\n",
    "- The weak learner is chosen to minimize the errors (residuals) made by the previous model, effectively correcting the model's mistakes.\n",
    "- The learning rate, which controls the contribution of each weak learner to the ensemble, can be adjusted to make the updates more conservative or aggressive.\n",
    "\n",
    "#### Combination of Weak Learners:\n",
    "\n",
    "- After each iteration, the predictions of the new weak learner are combined with the predictions of the previous ensemble.\n",
    "- The combination is done by adding the new weak learner's predictions (scaled by the learning rate) to the previous ensemble's predictions.\n",
    "- The ensemble's predictions are updated, aiming to reduce the residual errors from the previous iterations.\n",
    "#### Sequential Iteration:\n",
    "\n",
    "- Steps 2 and 3 are repeated for a predefined number of iterations or until a certain performance criterion is met.\n",
    "- With each iteration, the ensemble becomes more accurate in modeling the relationship between the input features and the target variable.\n",
    "#### Final Model Output:\n",
    "\n",
    "- The final prediction for a given input is the sum of the initial model's prediction and the predictions of all the weak learners in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0bbd52-3deb-43ee-abff-0c4628dde72b",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcce033-36a5-4722-badd-bf7b654d9817",
   "metadata": {},
   "source": [
    "#### Initialization:\n",
    "\n",
    "The ensemble starts with an initial model, often represented as F0(x), which is a simple approximation of the target variable. For regression tasks, this can be the mean of the target values.\n",
    "\n",
    "#### Residual Calculation:\n",
    "\n",
    "- In each iteration t, the algorithm calculates the residuals or errors of the current model's predictions. The residuals for example i are calculated as:\n",
    "#### ri**(t)=yi-Ft-1(Xi)\n",
    "\n",
    "- yi represents the actual target value for example \n",
    "- Fi-1(Xi) represents the ensemble's prediction up to iteration t-1 for example i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f938393-bfb7-40b3-9962-dcd40e77a594",
   "metadata": {},
   "source": [
    "#### Training a Weak Learner:\n",
    "\n",
    "- A new weak learner (e.g., a shallow decision tree) is trained to predict the residuals ri**t The goal is to find a weak learner ht(X) that  minimizes the loss function, which can be defined as:\n",
    "#### L(y,Ft-1(X)+ht(X))\n",
    "\n",
    "- Common loss functions include mean squared error (for regression) or logistic loss (for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6b7ca-9273-435b-8dc4-02676d2fc617",
   "metadata": {},
   "source": [
    "#### Weighted Combination of Weak Learners:\n",
    "\n",
    "- After training the new weak learner, its predictions ht(x) are combined with the predictions of the previous ensemble up to iteration t-1\n",
    "\n",
    "- The ensemble's prediction at iteration t becomes:\n",
    "  Ft(X)=Ft-1(X)+alpha(ht)(X)\n",
    "- alpha(t) is a weight assigned to the new weak learner, and it represents the contribution of the weak learner to the ensemble. It is determined during training and is based on the optimization of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fcfa44-03b0-4c9d-b37d-3cffad1ab5f5",
   "metadata": {},
   "source": [
    "#### Sequential Iteration:\n",
    "\n",
    "Steps 3 and 4 are repeated for a predefined number of iterations, with each iteration focusing on reducing the residuals and improving the ensemble's predictions.\n",
    "The ensemble gradually improves by learning to correct its previous errors and capture the remaining patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f47244-66fc-4f24-ae7b-67708616b594",
   "metadata": {},
   "source": [
    "#### Final Model Output:\n",
    "\n",
    "The final prediction of the Gradient Boosting ensemble is the sum of the initial model's prediction and the contributions of all the weak learners up to the final iteration:\n",
    "\n",
    "#### Ft(X)=F0(X)+sum(alpha(t)ht(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
